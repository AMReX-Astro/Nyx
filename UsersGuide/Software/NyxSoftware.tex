\section{Code structure}
The code structure in the NYX directory that you have checked out is the following:

\begin{itemize}
\item {\bf Parallel}   : the ``primary'' directory, all in C++/Fortran 
  \begin{itemize}
  \item {\bf amrlib}   : basic routines necessary for AMR 
  \item {\bf bndrylib} : basic interface routines
  \item {\bf BoxLib}   : the most basic directory, everything depends on classes defined here
  \item {\bf Nyx}   : this is where all the actual algorithm stuff lives
  \begin{itemize}
    \item {\bf Exec}       : various examples
    \begin{itemize}
      \item {\bf Test\_90Mpc}  : run directory for the 90Mpc box problem
    \end{itemize}
    \item {\bf Source}     : source code
    \item {\bf UsersGuide} : you're reading this now!
  \end{itemize}
  \item {\bf mglib}      : this is the MultiGrid solver written in C++/Fortran -- would be 
                           used for the Poisson solve if we didn't use the F90 solver
  \item {\bf MGT solver} : this is the interface between the C++ code and the F90 
                           multigrid solver
  \item {\bf mk}         : makefile stuff  for C++/Fortran
  \item {\bf pAmrvis}    : contains amrvis, a visualization tool for 2D and 3D plotfiles
  \item {\bf scripts}    : compiling stuff for C++/Fortran
  \item {\bf util}       : various data analysis utilities
  \item {\bf volpack}    : package required to compile and run amrvis in 3d
  \end{itemize}
\item {\bf fParallel} : the F90 directory, used here only for the multilevel Poisson solver,
                        EOS, and neworks.
  \begin{itemize}
  \item {\bf boxlib}  : the most basic directory which defines things for the F90 codes
  \item {\bf data processing} : this contains Fortran routines that read in Nyx plotfiles and can do
                                simply processing, including extracting a line along the $x$, $y$, or $z$-axis,
                                averaging a solution over spherical angles to get the profile as a function
                                of radius, and dumping out a brick of data that can be read by IDL -- see 
                                Mike Zingale if interested
  \item {\bf extern}  : contains EOS and networks
  \item {\bf mg}      : F90 multigrid -- used for the gravity solver only
  \item {\bf mk}      : makefile stuff for F90
  \item {\bf scripts} : compiling stuff for F90
  \end{itemize}
\end{itemize}

Within {\bf Parallel/Nyx} are the following files:

  \begin{itemize}
  \item {\bf Nyx.cpp}           : this holds the time advancement algorithm 
  \item {\bf Nyx\_setup.cpp}    : this is where components of the state, boundary 
                                  conditions, derived quantities, and error estimation 
                                  quantities are defined for a run
  \item {\bf MacBndry.cpp}      : this is needed to correctly do the adaptive boundary 
                                  conditions for the Poisson solver
  \item {\bf main.cpp}          : initializes the BoxLib and timing stuff properly -- don't 
                                  mess with this
\end{itemize}
\section{Variable Names}
The following is a list of variables, routines, etc used in NYX. It may not be complete or even entirely accurate; it's mostly intended for my own use.\\

{\bf lo,hi}: index extent of the "grid" of data currently being handled by a NYX routine\\

{\bf domlo, domhi}: index extent of the problem domain. This changes according to refinement level: 0th refinement level will have 0, castro.max\_grid\_size, and nth level will go from 0 to castro.max\_grid\_size*(multiplying equivalent of sum)castro.ref\_ratio(n).\\

{\bf dx}: cell spacing, presumably in cm, since CASTRO uses cgs units\\

{\bf xlo}: physical location of the lower left-hand corner of the "grid" of data currently being handled by a CASTRO routine\\

{\bf bc}: array that holds boundary condition of and array. Sometimes it appears of the form bc(:,:) and sometimes bc(:,:,:). The last index of the latter holds the variable index, i.e. density, pressure, species, etc.\\

\section{Parallel I/O}
Both checkpoint files and plotfiles are really directories containing subdirectories: 
one subdirectory for each level of the AMR hierarchy.  The fundamental data structure 
we read/write to disk is a MultiFab, which is made up of multiple FAB's, one FAB per grid.
Multiple MultiFabs may be written to each directory in a checkpoint file.  
MultiFabs of course are shared across CPUs; a single MultiFab may be 
shared across thousands of CPUs.  Each CPU writes the part of the MultiFab that it owns to disk, 
but they don't each write to their own distinct file.  Instead each MultiFab is written to a 
runtime configurable number of files N (N can be set in the inputs file as the
parameter {\bf amr.checkpoint\_nfiles} and {\bf amr.plot\_nfiles}; the default is 64).  
That is to say, each MultiFab is written to disk across at most N files, 
plus a small amount of data that gets written to a header file describing how the 
file is laid out in those N files.

What happens is N CPUs each opens a unique one of the N files into which the MultiFab is 
being written, seeks to the end, and writes their data.  The other CPUs are waiting at a 
barrier for those N writing CPUs to finish.  This repeats for another N CPUs until all the 
data in the MultiFab is written to disk.  All CPUs then pass some data to CPU 0 which writes 
a header file describing how the MultiFab is laid out on disk.

We also read MultiFabs from disk in a "chunky" manner opening only N files for reading at a 
time.  The number N, when the MultiFabs were written, does not have to match the number N when 
the MultiFabs are being read from disk.  Nor does the number of CPUs running while reading in 
the MultiFab need to match the number of CPUs running when the MultiFab was written to disk.

Think of the number N as the number of independent I/O pathways in your underlying parallel 
filesystem.  Of course a "real" parallel filesytem should be able to handle any reasonable 
value of N.  The value -1 forces N to the number of CPUs on which you're running, which means 
that each CPU writes to a unique file, which can create a very large number of files, which 
can lead to inode issues.
